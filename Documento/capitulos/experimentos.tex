\section{Entorno de prueba Mininet}

Para el diseño de esta emulación se tomó como referencia la familia de SmartNICs NVIDIA BlueField-2\cite{liu2021performancebluefield2}, ya que en una fase inicial del proyecto se contempló la posibilidad de utilizar una SmartNIC real de dicha familia ---concretamente la NVIDIA A30X Converged Accelerator---, opción que finalmente no pudo llevarse a cabo debido a una avería del hardware disponible.

El modo de operación previsto para dicha SmartNIC era el denominado \textit{loopback}\cite{cisco-smartnic-loopback}, en el cual el tráfico recibido por la primera interfaz de red es redirigido de forma obligatoria hacia la segunda interfaz, pasando previamente por la DPU, donde se ejecuta el programa XDP acoplado al driver de red \ref{red-real}. De este modo, los paquetes son procesados y devueltos al flujo de datos sin intervención de la CPU del host. Este esquema de funcionamiento es habitual en tareas de seguridad, como la implementación de firewalls o sistemas de inspección de tráfico, ya que permite ejecutar las funciones de filtrado de forma aislada y reducir la carga computacional del sistema anfitrión.

La adopción de este modelo resulta especialmente adecuada para el presente trabajo, dado que el árbol de decisión implementado puede considerarse un subtipo de firewall especializado. Al ejecutar la lógica de clasificación directamente sobre la ruta de datos y fuera del host, se favorece un diseño más eficiente y alineado con los principios de procesamiento en línea que caracterizan a las SmartNIC modernas.

\begin{figure}[Topología de Red Real]{red-real}{Diseño Real de la topología de red}
	\includegraphics[width=0.9\textwidth]{capturas/Diagrama_tarjeta_red.png}
\end{figure}

Teniendo en cuenta que el entorno de referencia real consiste en una tarjeta de red con dos puertos a nivel de enlace, se decidió simplificar la topología de Mininet con el objetivo de reproducir de forma más fiel el comportamiento de dicho escenario. La red utilizada consta únicamente de dos hosts conectados a un único switch, que actúa como punto de interconexión y emulación del camino de datos, eliminando la necesidad de routers adicionales o interfaces suplementarias, tal y como se muestra en la figura \ref{red-mininet-2}. Esta simplificación permite reducir la complejidad de la topología y minimizar factores externos que podrían afectar a las mediciones, como latencias artificiales introducidas por componentes adicionales de la red virtual.

Además, esta configuración contribuye a disminuir la variabilidad en los resultados experimentales y a reducir efectos secundarios propios de la emulación, como el tráfico de control asociado a la resolución de direcciones MAC mediante ARP. Al aproximarse al comportamiento funcional de un entorno con DPU en modo \textit{inline}, la topología proporciona un marco adecuado para evaluar el rendimiento de la red bajo la influencia del filtro basado en árbol de decisión. El siguiente paso en el proceso consiste en la realización de pruebas de rendimiento utilizando la herramienta iperf, lo que permitirá cuantificar de manera más precisa el impacto del filtrado sobre el tráfico de red.

\begin{figure}[Topología de Red Final]{red-mininet-2}{Diseño de topología de red usada en Mininet para pruebas.}
	\includegraphics[width=0.55\textwidth]{capturas/DiagramaRed2.png}
\end{figure}

En los fragmentos de código \ref{redarbolrw} y \ref{redarbolrw1} se muestran extractos de los ficheros redarbolrw.py y redarbolrw1.py, correspondientes a la definición de la topología preliminar y de la topología definitiva, respectivamente.

\PythonCode[redarbolrw]{Red Árbol Preliminar}{Fragmento de código que define la topología de red usada en las pruebas preliminares.}{../Mininet/redarbolrw.py}{22}{54}{22}

\PythonCode[redarbolrw1]{Red Árbol Final}{Fragmento de código que define la topología de red usada en las pruebas finales.}{../Mininet/redarbolrw1.py}{8}{25}{8}


\begin{comment}
% AQUÍ EMPIEZA AWS

\section{Entorno AWS}

AWS (Amazon Web Services) es, según los datos publicados por Statista\cite{statista-cloud-marketshare}, el servicio de computación en la nube más utilizado a nivel mundial, alcanzando una cuota de mercado del 29 \% en el tercer trimestre de 2025, siendo esta la información más reciente disponible en el momento de elaboración del presente trabajo.

Este proveedor es ampliamente reconocido por la flexibilidad de su plataforma, que permite desplegar arquitecturas heterogéneas y adaptadas a distintos escenarios de computación y redes. En el contexto de este proyecto, AWS se emplea con el objetivo de simular, de la manera más realista posible, una topología de red en la que estaría presente una SmartNIC, pero sin disponer físicamente de dicho dispositivo.

Para ello, se hace uso de varios componentes fundamentales de la plataforma. El primero de ellos es la VPC (Virtual Private Cloud)\cite{aws-vpc}, que constituye una red virtual aislada con un funcionamiento análogo al de las redes tradicionales de los centros de datos, con la particularidad de que se ejecuta sobre uno o varios centros de datos de AWS —en este caso, únicamente en una región con el fin de minimizar la latencia— y permanece completamente aislada del resto de infraestructuras de la nube. Dentro de una VPC es necesario definir una o más subredes, que representan rangos específicos de direcciones IP asociados a dicha red virtual\cite{aws-subnet}.

Finalmente, el elemento central utilizado en este trabajo son las instancias EC2 (Elastic Compute Cloud)\cite{aws-ec2}, que constituyen máquinas virtuales ejecutadas sobre el hardware físico de Amazon. AWS dispone de múltiples tipos de instancias EC2 optimizadas para diferentes casos de uso; la selección concreta realizada para este proyecto, así como su justificación técnica, será detallada posteriormente.

La topología de la nube AWS del proyecto consta ---tal y como se puede ver en la figura \ref{red-aws}--- de una única VPC que a su vez posee una única Subred, en ella se encuentran dos instancias EC2. La primera de ella es la encargada de emular el router y por tanto de enviar el tráfico para las pruebas, la segunda es la que emula la SmartNIC, esta posee dos interfaces de red, la primera es la encargada de recibir el tráfico de la primera instancia EC2, la segunda emula la interfaz de red asociada a la DPU de la Smart NIC y es en la que se va a ejecutar el filtro XDP.

\begin{figure}[Topología de Red AWS]{red-aws}{Diseño de topología de red usada en AWS para pruebas.}
	\includegraphics[width=1 \textwidth]{capturas/Diagrama_AWS_SmartNIC.png}
\end{figure}

La selección del tipo de instancia EC2 utilizada en cada caso se ha realizado en función de los requisitos específicos del rol que debían desempeñar dentro de la topología experimental. En primer lugar, para la instancia destinada a emular el router se optó por el tipo t3.micro\cite{aws-t3-micro}, dado que se trata de una instancia genérica, ligera, con recursos computacionales moderados y un coste reducido, características que se alinean adecuadamente con el comportamiento habitual de los routers convencionales en entornos reales.

La elección de la instancia encargada de simular el comportamiento de una SmartNIC resultó más compleja, ya que fue necesario considerar varios criterios técnicos. En primer lugar, debía tratarse de un tipo de instancia compatible con XDP o que, con modificaciones mínimas, pudiera ofrecer dicha compatibilidad. En segundo lugar, era imprescindible garantizar que el hipervisor asociado introdujera la menor latencia posible, dado que la demora añadida por la virtualización podría distorsionar los resultados del experimento. Por último, era necesario disponer de una capacidad de cómputo comparable a la de una SmartNIC real ---o ligeramente superior, para compensar la latencia residual del hipervisor--- evitando, en cualquier caso, seleccionar una instancia con recursos excesivamente elevados que desvirtuaran la simulación.

Atendiendo a estos criterios, se seleccionó la instancia c6gn.large\cite{aws-c6gn-large} para emular la SmartNIC. Este tipo de instancia se ejecuta sobre la arquitectura AWS Nitro v4\cite{aws-nitro}, la cual desplaza parte de las funciones del hipervisor a hardware dedicado, reduciendo significativamente la latencia y ofreciendo un entorno más adecuado para la ejecución de tecnologías de bajo nivel como XDP.

La instancia EC2 empleada para emular el comportamiento de una SmartNIC dispone de dos interfaces de red conectadas mediante Ethernet, de forma análoga a la arquitectura presente en la mayoría de las SmartNIC comerciales. Para el diseño de esta emulación se tomó como referencia la familia de SmartNICs NVIDIA BlueField-2\cite{liu2021performancebluefield2}, ya que en una fase inicial del proyecto se contempló la posibilidad de utilizar una SmartNIC real de dicha familia ---concretamente la NVIDIA A30X Converged Accelerator---, opción que finalmente no pudo llevarse a cabo debido a una avería del hardware disponible.

El modo de operación previsto para dicha SmartNIC era el denominado \textit{loopback}\cite{cisco-smartnic-loopback}, en el cual el tráfico recibido por la primera interfaz de red es redirigido de forma obligatoria hacia la segunda interfaz, pasando previamente por la DPU. De este modo, los paquetes son procesados y devueltos al flujo de datos sin intervención de la CPU del host. Este esquema de funcionamiento es habitual en tareas de seguridad, como la implementación de firewalls o sistemas de inspección de tráfico, ya que permite ejecutar las funciones de filtrado de forma aislada y reducir la carga computacional del sistema anfitrión.

La adopción de este modelo resulta especialmente adecuada para el presente trabajo, dado que el árbol de decisión implementado puede considerarse un subtipo de firewall especializado. Al ejecutar la lógica de clasificación directamente sobre la ruta de datos y fuera del host, se favorece un diseño más eficiente y alineado con los principios de procesamiento en línea que caracterizan a las SmartNIC modernas.

% AQUÍ ACABA AWS
\end{comment}

\section{Herramientas}

En la presente sección se describen las herramientas empleadas tanto para la generación de tráfico como para la obtención de métricas asociadas. Si bien todas ellas persiguen el mismo objetivo general —evaluar el rendimiento de la red bajo distintas condiciones de carga—, difieren en el tipo de tráfico que generan y en su origen. Esta complementariedad justifica la utilización de las tres, ya que permite llevar a cabo un análisis más completo y ofrecer una visión más precisa del comportamiento del sistema.

\begin{comment}
\subsection{Iperf}

Iperf 2 es ---como indica su manual\cite{iperf2-manual}--- una herramienta de testeo que emplea sockets de red para realizar medidas de tráfico. Solo acepta los protocolos TCP y UDP, y permite evaluar métricas de rendimiento como el \textit{throughput} y la latencia. Entre sus funcionalidades se encuentra la posibilidad de generar tráfico unidireccional, full duplex (usando un mismo socket) y bidireccional, además de soportar múltiples flujos simultáneos de manera concurrente. Asimismo, admite tráfico multicast, incluyendo uniones de tipo source-specific multicast (SSM). Su diseño multihilo posibilita alcanzar un rendimiento elevado en las pruebas, mientras que las métricas obtenidas permiten caracterizar el desempeño de la red de extremo a extremo. Para la ejecución de un test es necesario establecer tanto un servidor, que recibe el tráfico, como un cliente, que lo genera y envía; normalmente se ubican en equipos distintos, aunque no es estrictamente necesario.

La elección de Iperf como herramienta de apoyo en la experimentación se justifica principalmente por su simplicidad y facilidad de uso. Esta característica permite diseñar pruebas controladas en entornos ideales, utilizando los dos protocolos de transporte más empleados en la actualidad en el ámbito de las redes, TCP y UDP. Además, la herramienta proporciona de manera integrada métricas fundamentales como ancho de banda, latencia y pérdida de paquetes, las cuales pueden obtenerse bajo diferentes configuraciones sin añadir complejidad al proceso experimental. Gracias a estas propiedades, Iperf resulta especialmente útil para establecer una línea base de rendimiento y evaluar de forma genérica el impacto que introduce el filtro basado en árboles de decisión sobre el tráfico de red.
\end{comment}

\subsection{TCPReplay}

TCPReplay es una herramienta diseñada para el reenvío de paquetes de red previamente capturados mediante utilidades como tcpdump o Wireshark. Su principal funcionalidad consiste en reproducir dichos paquetes, bien respetando la velocidad original con la que fueron capturados, o bien a una velocidad definida por el usuario. En este último caso, la herramienta permite ajustar de manera explícita el ritmo de envío, ya sea en función de paquetes por segundo (pps) o de la tasa de transmisión expresada en megabits por segundo (Mbps)\cite{tcpreplay-manpage}.

TCPReplay aporta dos ventajas clave para la evaluación experimental. En primer lugar, permite reproducir tráfico real contenido en ficheros PCAP, lo que posibilita realizar ensayos más representativos que los basados únicamente en tráfico sintético —por ejemplo, incluyendo protocolos distintos de TCP/UDP (ICMP, ARP, SMB, etc.) y secuencias de paquetes reales—. En segundo lugar, ofrece un control fino sobre la velocidad de reproducción (tanto en paquetes por segundo como en Mbps y otras modalidades), lo que facilita la emulación de distintos escenarios de carga de red (desde tráfico esporádico hasta condiciones de estrés). Estas capacidades hacen de TCPReplay una herramienta especialmente adecuada para validar el comportamiento del filtro XDP frente a tráfico heterogéneo y para estudiar su rendimiento bajo diferentes perfiles de carga.

\subsection{Script de Reenvio}

Una limitación de \textit{tcpreplay} cuando se utiliza de forma aislada es que no permite emular de manera realista un escenario de ataque, ya que en este tipo de situaciones el sistema suele recibir simultáneamente tráfico benigno y tráfico malicioso. Por este motivo, se desarrolló un script propio ---se puede encontrar en \verb|Mininet/script_reenvio.py|--- que hace uso de \textit{tcpreplay} con el objetivo de reproducir un escenario más representativo, combinando ambos tipos de tráfico en una misma ejecución.

El funcionamiento del script se divide en dos fases. En primer lugar, se realiza una fase de preparación en la que se genera un fichero PCAP auxiliar, el cual se elimina una vez finalizada la prueba. Dicho fichero se construye a partir de fragmentos de los PCAP legítimo y malicioso proporcionados como entrada. Durante esta fase, la selección de paquetes se realiza de manera aleatoria: en cada iteración se escoge al azar si se extraerán paquetes del PCAP legítimo o del malicioso, tras lo cual se leen y escriben 100 paquetes consecutivos de la captura seleccionada. Este proceso se repite hasta alcanzar el número total de paquetes que se pretende transmitir, calculado como el producto de la tasa objetivo de paquetes por segundo y la duración del experimento, añadiéndose adicionalmente un 20\% de margen para garantizar que el fichero auxiliar contenga suficientes paquetes ante posibles variaciones o imprevistos durante la reproducción.

Una vez generado el PCAP auxiliar, se ejecuta la segunda fase, consistente en reenviar dicho fichero a la tasa y duración especificadas como parámetros de entrada por el usuario, utilizando nuevamente \textit{tcpreplay}. Inicialmente se consideró implementar esta reproducción directamente mediante \textit{Scapy}, seleccionando los paquetes de forma aleatoria en tiempo de ejecución sin generar un PCAP previo. No obstante, esta aproximación se descartó debido a que, en pruebas preliminares, no fue posible alcanzar tasas de paquetes por segundo suficientemente elevadas para el propósito experimental del presente trabajo.

\subsection{TCPDump}

Tanto TCPReplay como el script desarrollado en este trabajo no disponen de funcionalidades propias para medir el rendimiento de la red, limitándose únicamente al reenvío de paquetes. Por este motivo, resulta necesario complementar su uso con herramientas de captura de tráfico, como TCPDump, que permiten registrar los paquetes transmitidos y analizar posteriormente métricas relevantes como el \textit{throughput}, la latencia o la pérdida de paquetes\cite{tcpdump-manpage}.

TCPDump es una herramienta de línea de comandos que permite capturar y visualizar paquetes de red en tiempo real. Es capaz de filtrar el tráfico en función de distintos criterios, como direcciones IP, puertos o protocolos, y de guardar las capturas en ficheros para su análisis posterior. Además, TCPDump permite examinar cabeceras de protocolos como Ethernet, IP, TCP y UDP, proporcionando información detallada sobre el flujo de datos entre hosts, lo que lo convierte en un recurso fundamental para evaluar cómo afectan mecanismos de filtrado, como el árbol de decisión XDP, al rendimiento de la red.


\section{Métricas Relevantes}

Para poder analizar el rendimiento del sistema, tanto con el árbol de decisión activo como con este inactivo, es necesario centrarse en una serie de métricas relevantes. Estas métricas permitirán cuantificar de manera objetiva el impacto que el árbol de decisión tiene sobre el comportamiento general del sistema, proporcionando una base sólida para la comparación entre ambos escenarios.

La primera métrica a considerar es el rendimiento o \textit{throughput}, el cual mide la cantidad de información útil que se recibe por segundo. Dado que se trata de un entorno de red, el estándar es expresar esta métrica en bits por segundo (bps) en lugar de utilizar unidades como bytes por segundo u otras equivalentes, ya que permite una representación más precisa y ampliamente aceptada en el ámbito de las comunicaciones de red.

\begin{equation}[eq:throughput]{Cálculo del throughput}
  \text{Throughput (bps)} = \frac{8 \times B_{\text{recibidos}}}{T}
\end{equation}

Además de medir el rendimiento en bits por segundo, resulta de interés cuantificar también el número de paquetes por segundo (PPS) que el sistema es capaz de procesar. Aunque a primera vista esta métrica pueda parecer redundante respecto a la anterior, ambas reflejan aspectos distintos del comportamiento del sistema. En efecto, una misma cantidad total de información puede estar dividida en un número diferente de paquetes, y cada uno de ellos requiere una serie de operaciones de procesamiento que deben ejecutarse de manera independiente --como la lectura de cabeceras, la evaluación de condiciones o el acceso a estructuras de datos internas--. Por este motivo, el rendimiento medido en paquetes por segundo permite identificar posibles cuellos de botella asociados al manejo intensivo de paquetes pequeños y evaluar con mayor precisión el impacto del árbol de decisión sobre la carga computacional del sistema.

\begin{equation}[eq:pps]{Cálculo de paquetes por segundo}
	\text{PPS} = \frac{N_{\text{paquetes}}}{t_{\text{fin}}-t_{\text{inicio}}}
\end{equation}

Otra métrica fundamental en un entorno de red es el porcentaje de pérdida de paquetes (\textit{packet loss rate}). Este parámetro resulta esencial para evaluar la fiabilidad y eficiencia de un sistema, ya que un alto rendimiento en términos de bits por segundo (bps) no necesariamente implica un funcionamiento óptimo si el sistema no es capaz de procesar o asimilar correctamente todo el tráfico recibido. En este sentido, un porcentaje elevado de pérdida puede indicar cuellos de botella, saturación del sistema o limitaciones en la capacidad de procesamiento del tráfico, comprometiendo la eficacia global de la solución.

\begin{equation}[eq:perdida]{Cálculo del porcentaje de pérdida}
  \text{Pérdida (\%)} = 100 \times \frac{N_{\text{enviados}} - N_{\text{recibidos}}}{N_{\text{enviados}}}
\end{equation}

No obstante, la interpretación de esta métrica requiere un análisis diferenciado cuando el filtro basado en el árbol de decisión se encuentra activo. En este caso, parte de los paquetes son descartados de manera intencionada por el propio filtro al ser clasificados como potenciales amenazas. Por tanto, resulta fundamental distinguir entre los paquetes perdidos debido a posibles cuellos de botella generados al añadir una nueva capa de procesamiento y aquellos paquetes que son eliminados de manera deliberada por el filtro como parte de su funcionamiento normal. Esta distinción permite evaluar con precisión tanto la eficiencia del sistema como la eficacia del mecanismo de filtrado implementado.

\begin{equation}[eq:perdida arbor]{Cálculo del porcentaje de pérdida con árbol activo}
  \text{Pérdida Real(\%)} = 100 \times \frac{N_{\text{enviados}} - N_{\text{recibidos}} - N_{\text{filtrados}}}{N_{\text{enviados}}}
\end{equation}

Existe además la métrica denominada \textit{goodput}, la cual representa el rendimiento efectivo de una muestra de tráfico expresado en paquetes por segundo (PPS), considerando únicamente aquellos paquetes que son entregados con éxito y excluyendo las pérdidas producidas durante la transmisión.

\begin{equation}[eq:goodput]{Cálculo del rendimiento útil \(goodput\)}
	\text{Goodput} = \text{pps\_medidos} \times (1 - \frac{\text{porcentaje\_perdida\_real}}{100})
\end{equation}

Por último, otras dos métricas relevantes en el análisis experimental son el porcentaje de uso de la CPU y el porcentaje de uso de la memoria RAM. Estas métricas permiten evaluar el impacto del filtro sobre los recursos computacionales del sistema y determinar si su ejecución introduce un cuello de botella o un factor limitante asociado al consumo de hardware, especialmente en escenarios de alta carga de tráfico.

\section{Metodología Seguida}

Antes de llevar a cabo los experimentos de rendimiento, el primer paso consiste en verificar si el árbol de decisión entrenado presenta una tasa de acierto real lo suficientemente adecuada como para considerar que, con un entrenamiento más exhaustivo, podría emplearse como un filtro efectivo. En este trabajo se estableció como umbral mínimo deseable una tasa de acierto real del 60\%, dado que el objetivo principal no es evaluar la eficacia del modelo, sino analizar el impacto que dicho filtro puede tener sobre el rendimiento del sistema.

Las pruebas realizadas en este ámbito, como la que se muestra en la figura \ref{prueba_calidad_arbol}, han arrojado una tasa de acierto situada entre el 65\% y el 70\%. Esta tasa se ha calculado como el cociente entre el número de paquetes maliciosos correctamente rechazados y el número total de paquetes maliciosos enviados, multiplicado por cien. Con ello se confirma que el árbol utilizado posee una calidad suficiente para servir como base en los experimentos de rendimiento desarrollados en este trabajo.

\begin{figure}[Prueba tasa de acierto del Árbol]{prueba_calidad_arbol}{Prueba realizada para comprobar la tasa de acierto real del árbol de decisión.}
	\includegraphics[width=1.1 \textwidth]{capturas/Prueba_calidad_arbol.png}
\end{figure}

El experimento consiste en generar tráfico de red a una tasa determinada durante un intervalo de tiempo fijo, ejecutando primero las pruebas con el filtro desactivado y, posteriormente, repitiéndolas con el filtro XDP activado. En concreto, se realizan iteraciones con tasas comprendidas entre 64 ($2^6$) y 16.384 ($2^{14}$) paquetes por segundo, manteniendo una duración de 10 segundos por ejecución. Este procedimiento se aplica de forma independiente para cada herramienta empleada —cuyo papel y objetivo se describirán en párrafos posteriores—. Tras cada iteración, se registran las métricas consideradas más relevantes, con la excepción del rendimiento en bits por segundo (bps), cuya obtención y tratamiento se detalla más adelante.

Para ejecutar estas pruebas de manera reproducible, se empleó un único script en Python encargado de automatizar tanto el despliegue del entorno como la ejecución completa del conjunto de iteraciones. En primer lugar, el script inicializa la topología en Mininet y obtiene los identificadores de proceso (PID) asociados a los hosts \textit{hsrc} y \textit{hdst}, lo que permite lanzar comandos directamente sobre cada host emulado. A continuación, se ejecuta el bloque de iteraciones con el filtro XDP desactivado, incorporando un breve \textit{sleep} entre ejecuciones. Una vez finalizado este bloque, se introduce una pausa más prolongada con el objetivo de reducir el posible efecto residual de las pruebas previas sobre las siguientes mediciones. Posteriormente, el script compila y activa el filtro XDP y repite el mismo rango de iteraciones bajo condiciones equivalentes.

Finalmente, tras cada iteración —tanto con el filtro activo como desactivado— el script vuelca las métricas registradas a un fichero CSV asociado a dicha prueba. Los resultados completos obtenidos durante la experimentación pueden consultarse en el directorio \verb|ResultadoExperimento| del repositorio.

Con el fin de validar los resultados obtenidos en la experimentación y poder extraer conclusiones fundamentadas, se han elaborado diversas gráficas orientadas a responder a una serie de cuestiones específicas. Estas representaciones, junto con el análisis asociado, pueden agruparse en cuatro bloques principales: capacidad y escalabilidad del envío, pérdidas de paquetes, actividad del filtro y coste del sistema.

Comenzando por el primer bloque, relativo a la capacidad y escalabilidad del envío, se han seleccionado las figuras \verb|pps_measured vs pps_target| (\ref{measuredvstarget}) y \verb|goodput_pps vs pps_target| (\ref{goodput}). La primera permite comprobar si el entorno experimental es capaz de generar y transmitir el tráfico solicitado de forma consistente, evaluando el grado de correspondencia entre la tasa objetivo y la tasa efectivamente medida. La segunda, siguiendo la misma línea, permite analizar el rendimiento útil (goodput) en función de la tasa esperada, estimando la tasa efectiva de entrega correcta al considerar únicamente los paquetes entregados sin pérdida real. En conjunto, ambas representaciones permiten validar la consistencia del entorno de pruebas y detectar posibles fenómenos de saturación cuando la tasa alcanzada deja de crecer proporcionalmente a medida que aumenta la carga.

\begin{figure}[pps\_measured vs pps\_target]{measuredvstarget}{Gráfica que muestra la comparativa entre los pps medidos y los esperados.}
	\includegraphics[width=1\textwidth]{capturas/03_pps_measured.png}
\end{figure}

\begin{figure}[goodput\_pps vs pps\_target]{goodput}{Gráfica que muestra la comparativa entre rendimiento útil y los pps esperados.}
	\includegraphics[width=1\textwidth]{capturas/07_goodput.png}
\end{figure}

En el segundo bloque, centrado en el análisis de pérdidas, se han seleccionado las figuras \verb|lost_percent vs pps_target| (\ref{lossaparente}) y \verb|lost_real_percent vs pps_target| (\ref{lossreal}). La primera representa el porcentaje de pérdida medido globalmente durante cada iteración, incluyendo tanto pérdidas debidas a limitaciones del entorno de transmisión como paquetes descartados intencionadamente por el filtro cuando este está activo. La segunda, por su parte, muestra el porcentaje de pérdida real, calculado descontando del total los paquetes asociados al filtrado (\verb|XDP_DROP|), lo que permite cuantificar la degradación efectiva de la comunicación debida a pérdidas no intencionadas. Consideradas conjuntamente, ambas figuras permiten diferenciar entre una pérdida aparente elevada provocada por el funcionamiento normal del filtro y una pérdida real asociada a degradación de rendimiento, proporcionando una interpretación más precisa del impacto del filtrado sobre el rendimiento de la red.

\begin{figure}[lost\_percent vs pps\_target]{lossaparente}{Gráfica que muestra la comparativa entre el porcentaje de pérdida aparente y los pps esperados.}
	\includegraphics[width=1\textwidth]{capturas/01_loss_aparente.png}
\end{figure}

\begin{figure}[lost\_real vs pps\_target]{lossreal}{Gráfica que muestra la comparativa entre el porcentaje de pérdida real y los pps esperados.}
	\includegraphics[width=1\textwidth]{capturas/02_loss_real.png}
\end{figure}

En el tercer bloque, orientado a caracterizar la actividad del filtro, se han seleccionado las figuras \verb|paquetes_filtrados vs pps_target| (\ref{filtrados}) y \verb|loss_gap vs pps_target| (\ref{gaploss}). La primera muestra el número de paquetes descartados por el filtro, correspondiente al contador de eventos \verb|XDP_DROP|, y permite comprobar de forma directa que el mecanismo de filtrado se encuentra actuando durante la ejecución del experimento. La segunda representa la diferencia entre la pérdida aparente y la pérdida real, cuantificando el porcentaje de pérdida observado que se explica por el filtrado intencionado y no por degradación de la comunicación. En conjunto, ambas figuras permiten interpretar correctamente el incremento de pérdidas observado con el filtro activado, distinguiendo entre descartar tráfico de forma deliberada y pérdidas atribuibles a limitaciones del entorno de red o del generador de tráfico.

\begin{figure}[filtrados vs pps\_target]{filtrados}{Gráfica que muestra la comparativa entre los paquetes filtrados y los pps esperados.}
	\includegraphics[width=1\textwidth]{capturas/04_filtrados.png}
\end{figure}

\begin{figure}[lost\_real vs pps\_target]{gaploss}{Gráfica que muestra la diferencia entre la pérdida aparente y la pérdida real (gaploss) respecto a los pps esperados.}
	\includegraphics[width=1\textwidth]{capturas/08_gap_loss.png}
\end{figure}

Por último, el cuarto bloque analiza el coste computacional asociado al experimento, empleando las figuras \verb|cpu_usage_percent vs pps_target| (\ref{cpu}) y \verb|mem_usage_percent vs pps_target| (\ref{ram}). Estas representaciones permiten observar la evolución del consumo de recursos a medida que aumenta la carga de tráfico, comparando el comportamiento con el filtro desactivado y activado. De este modo, se evalúa si el filtrado introduce una sobrecarga sostenida que pudiera constituir un cuello de botella en el sistema, o si las variaciones observadas responden principalmente a la propia generación y transmisión del tráfico en el entorno de pruebas. Este análisis complementa las métricas de rendimiento de red, proporcionando una visión global del impacto del filtrado tanto en términos de entrega de paquetes como de uso de recursos.

\begin{figure}[CPU]{cpu}{Gráfica que muestra la comparativa entre el uso de la CPU y los pps esperados.}
	\includegraphics[width=1\textwidth]{capturas/05_cpu.png}
\end{figure}

\begin{figure}[RAM]{ram}{Gráfica que muestra la comparativa entre el uso de la memoria RAM y los pps esperados.}
	\includegraphics[width=1\textwidth]{capturas/06_ram.png}
\end{figure}

Aunque el \textit{throughput} en bits por segundo es una métrica habitual en la evaluación del rendimiento de red, en este trabajo no se incluyó su medición experimental directa. Esto se debe, en primer lugar, a que dicha métrica no puede aplicarse de manera sencilla en escenarios basados en tcpreplay, ya que no se dispone del tamaño en bits de cada paquete en el momento de su reenvío. Calcular este valor dinámicamente durante la transmisión introduciría un overhead adicional suficientemente elevado como para alterar de forma apreciable la propia métrica que se pretende medir.

Como alternativa, se planteó el uso de iperf para la obtención del \textit{throughput} en bits por segundo. Sin embargo, durante las pruebas preliminares se observó que, con el árbol de decisión activo en XDP, las mediciones proporcionadas por esta herramienta resultaban gravemente distorsionadas y poco representativas del comportamiento real del sistema. Por este motivo, se decidió no incluir esta métrica en los experimentos finales y centrar el análisis en métricas basadas en paquetes por segundo, pérdidas de paquetes y consumo de recursos, que permiten una evaluación más fiable del impacto del filtrado.
