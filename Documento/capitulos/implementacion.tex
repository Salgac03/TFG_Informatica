\section{Arquitectura de la propuesta}

En esta sección se ofrece una visión global de la arquitectura propuesta, describiendo los componentes principales que la conforman y el flujo de información entre ellos. Para facilitar la comprensión, se ha elaborado el diagrama mostrado en la figura \ref{diagrama_arquitectura}, que servirá como referencia a lo largo de este capítulo. Dicho diagrama proporciona una perspectiva general de la integración entre los distintos módulos, sin entrar todavía en los detalles específicos de implementación, los cuales se desarrollan en la sección siguiente.

La arquitectura de la implementación se organiza en tres bloques principales. El primero corresponde al tratamiento de los datos y el entrenamiento del árbol de decisión, mientras que el segundo se centra en la autogeneración del código XDP y el tercero en su posterior ejecución.

En la primera fase, se seleccionan los conjuntos de paquetes que serán utilizados para el entrenamiento del árbol de decisión. Estos datos son procesados en el espacio de usuario mediante un script en Python, encargado de transformar la información en un fichero CSV apto para su uso en el proceso de entrenamiento. A continuación, otro script en Python utiliza dicho fichero CSV para entrenar el árbol de decisión, generando así un modelo inicial.

En la segunda fase, el mismo script aplica un conjunto de reglas de traducción con el objetivo de convertir el árbol de decisión resultante en una serie de condiciones expresadas en código C restringido, compatibles con eBPF/XDP. Posteriormente, estas condiciones se integran en una plantilla Jinja, que permite generar automáticamente el programa XDP completo, el cual incluye tanto la lógica de clasificación derivada del árbol como el resto de estructuras auxiliares necesarias para su correcta ejecución.

La tercera fase corresponde a la ejecución del sistema. En este punto, el proceso principal consiste en la utilización de un Makefile, encargado de automatizar la compilación y despliegue del entorno. Dicho Makefile genera, por un lado, el bytecode del programa XDP, y por otro, el ejecutable del programa en espacio de usuario. Este último tiene como funciones tanto la carga del bytecode en el kernel del sistema como la monitorización en tiempo real de los paquetes que son descartados o aceptados en función de las reglas definidas por el árbol de decisión.

\begin{figure}[Arquitectura Implementación]{diagrama_arquitectura}{Arquitectura de generación de filtros eBPF}
	\includegraphics[width=0.8\textwidth]{capturas/Arquitectura_implementacion.png}
\end{figure}

Los componentes de la arquitectura se organizan en tres módulos principales. El primero corresponde al tratamiento de datos y a su conversión desde paquetes de red a un fichero CSV. El segundo módulo está constituido por el árbol de decisión, entrenado mediante la biblioteca scikit-learn de Python. Finalmente, el tercer módulo corresponde al código XDP, el cual incluye una parte autogenerada a partir del árbol de decisión. Cabe destacar el fichero \verb|DecisionTree/arbolrw.py|, encargado tanto del entrenamiento del árbol de decisión como de la autogeneración del código C, apoyándose para ello en una plantilla Jinja.

\section{Tratamiento de datos}

El conjunto de datos empleado para el entrenamiento del árbol de decisión se ha generado a partir de un fichero CSV de 294,6 MB elaborado por el autor. Para su creación, se utilizó la versión más reciente del script pcap2csv.py, el cual permite la unión de dos o más ficheros PCAP en un único fichero CSV, así como la diferenciación entre paquetes de tráfico malicioso y legítimo mediante el uso de las banderas -l y -m. El script almacena únicamente información de los paquetes hasta el nivel de aplicación, sin analizar el contenido de este nivel, debido a la complejidad que supone obtener dichos datos en C a nivel de kernel.

El proceso de conversión comienza definiendo un diccionario con todas las posibles características que puede tener un paquete de red. A continuación, se leen los paquetes uno a uno y cada uno se procesa mediante una función que devuelve un diccionario en el que cada clave corresponde al nombre de una columna y cada valor al dato asociado a ese paquete en particular. En caso de que un paquete carezca de alguno de los campos —por ejemplo, los relativos a IP—, dichos valores se registran como None en lugar de omitirse, con el fin de mantener un formato uniforme en el CSV y evitar errores durante el procesamiento posterior.

Una vez transformado el paquete en diccionario, se añade al final la etiqueta ``Yes'' si corresponde a una muestra de ransomware, o ``No'' en caso contrario. Posteriormente, el diccionario se incorpora a un array. Cuando dicho array alcanza los 1024 elementos, su contenido se vuelca al fichero CSV y se vacía para liberar memoria. Este procedimiento se repite hasta procesar todos los paquetes, momento en el que se realiza un último volcado con los datos restantes. Se ha optado por este método de lectura y escritura para optimizar tanto el uso de memoria como la frecuencia de accesos de escritura al disco.

Los ficheros PCAP correspondientes al tráfico de ransomware fueron obtenidos, tal y como se explica en el apartado 2.1 por el trabajo de Eduardo Berrueta et al. entre los años 2015 y 2022\cite{qnyn-q136-20}. Por otro lado, el fichero PCAP con tráfico legítimo fue generado por el autor mediante la captura de tráfico durante una jornada laboral en un laboratorio universitario, registrando el uso normal de los dispositivos y recogiendo tráfico de protocolos como HTTP, HTTPS, DNS, SSH, entre otros. Con el objetivo de incrementar la precisión del árbol de decisión, se generó adicionalmente tráfico SMB2 mediante la copia de un fichero de gran tamaño a una unidad de almacenamiento ubicada en el CPD de la universidad.

En el proceso de preparación del \textit{dataset}, se eliminaron las direcciones IP, las direcciones MAC y el timestamp de los paquetes, debido al número limitado presente tanto en las capturas de tráfico malicioso como en las de tráfico legítimo. Con ello, se evitó que el árbol de decisión se apoyara en características poco representativas o irrelevantes para un entorno real. 

\section{Árbol de decisión}

En esta subsección se detalla el proceso de configuración y entrenamiento del árbol de decisión utilizado en el presente trabajo. Se explican los criterios adoptados para su construcción, las particularidades detectadas durante el desarrollo y las adaptaciones realizadas en función de los resultados obtenidos.

Como métrica de calidad se mantuvo el índice de Gini, dada su fiabilidad y buen rendimiento, y como estrategia de división se empleó el criterio best. Aunque se preveía que el aumento del tamaño del \textit{dataset} respecto a las pruebas pudiera incrementar el tiempo de entrenamiento, en la práctica no se observó un impacto significativo. En caso contrario, se habría considerado cambiar la estrategia a random. En cuanto a la profundidad máxima del árbol, se fijó inicialmente en cuatro niveles, ya que en pruebas previas con otros conjuntos de datos no se apreciaron mejoras de precisión al superar este valor.

Para entrenar el árbol de decisión, una vez definidas las columnas que serían eliminadas, se estableció la variable \verb|x| como el conjunto de columnas correspondientes a los datos de entrada y la variable \verb|y| como la columna asociada al clasificador. A continuación, mediante la función \verb|train_test_split|, se dividió el conjunto de datos de forma aleatoria, asignando el 80\% de las filas al entrenamiento y el 20\% restante a las pruebas, con el fin de evaluar la fiabilidad del modelo. Una vez realizada esta partición, se procedió a entrenar el árbol utilizando los parámetros seleccionados y los datos de entrenamiento. Finalmente, se calculó la exactitud del modelo, si bien este valor debe interpretarse con cautela, dado que no siempre refleja de forma sólida la capacidad de generalización del clasificador. El código correspondiente al entrenamiento del árbol puede consultarse en el fragmento de código \ref{entrenamientoArbol}.

Durante el desarrollo se identificó un error en la configuración del entrenamiento: la variable \verb|eth_type| había sido incluida por error en la lista de columnas excluidas. Una vez corregida esta omisión y tras reentrenar el modelo, se observó que la precisión global se mantenía estable, aunque con una diferencia relevante en la complejidad del árbol, ya que la profundidad máxima alcanzada fue de tres niveles en lugar de cuatro, a pesar de que el límite establecido seguía siendo cuatro. Este comportamiento sugiere que la incorporación de \verb|eth_type| aporta capacidad predictiva al modelo, permitiendo una estructura más optimizada sin comprometer la precisión.

Posteriormente, se decidió eliminar la variable TTL al comprobar que resultaba tan poco fiable como las direcciones IP, introduciendo un nivel elevado de variabilidad. Tras esta modificación, el árbol resultante mostró una mayor amplitud, alcanzando ahora los cuatro niveles de profundidad. Aunque estas observaciones pudieran parecer superficiales, resultan de especial relevancia, ya que tanto el desarrollo del código como la generación automática de este se fundamentan en los árboles de decisión obtenidos, y en la evolución observada de uno respecto al otro.

\PythonCode[entrenamientoArbol]{Entrenamiento Árbol}{Fragmento de código de arbolrw.py encargado de entrenar el árbol de decisión.}{../DecisionTree/arbolrw.py}{103}{114}{114}

\section{Código XDP y Código autogenerado}

Tal y como se indicó en el apartado 2.2, los programas XDP constan de dos componentes diferenciados. El primero de ellos es un programa escrito en C que se compila de forma convencional y cuya ejecución tiene lugar íntegramente en el espacio de usuario. El segundo componente corresponde a un programa que se traduce a bytecode y que, posteriormente, es cargado por el primero en el espacio del kernel, donde finalmente se ejecuta.

El programa de usuario inicia comprobando que se haya especificado la interfaz de red sobre la que se ejecutará el programa XDP y verificando que dicha interfaz exista efectivamente en el equipo. A continuación, mediante la función \verb|bpf_object__open_file|, el programa analiza el objeto XDP y prepara las estructuras necesarias en el espacio de usuario, como los mapas. Asimismo, se crea en memoria un \verb|struct bpf_object|, desde el cual se podrá acceder a estas estructuras en el espacio de usuario. Una vez preparado el entorno de usuario, se carga el bytecode correspondiente en el espacio de kernel mediante la función \verb|bpf_object__load|.

El programa C aún requiere el descriptor de archivo (file descriptor, FD) del mapa para poder interactuar con él, el cual se obtiene a través de la función \verb|bpf_object__find_map_fd_by_name|, que recibe como parámetros la estructura previamente creada y el nombre del mapa definido en el programa XDP. Por seguridad y persistencia, se fija el mapa en \verb|/sys/fs/bpf/| mediante la función \verb|bpf_obj_pin|; esto permite acceder al mapa incluso si el proceso que lo creó finaliza de forma inesperada y, además, sirve para comprobar si el mapa ya existe, lo que indica que el programa se ha ejecutado previamente o se encuentra en ejecución en ese momento. Como última tarea para terminar de cargar el programa XDP se obtiene el descriptor de archivo de la función principal del programa XDP a través de la función \verb|bpf_program__fd|, tras esto se adjunta dicha función al \textit{hook} XDP que es el controlador de la gráfica a través de la función \verb|bpf_set_link_xdp_fd|. Una vez cargado el programa XDP, el programa de usuario se dedica a monitorizar los cambios del mapa y a mostrarlos en pantalla, dicho mapa contiene tanto el número de paquetes descartados como el número de paquetes que el programa XDP ha permitido pasar. En el fragmento de código \ref{xdp_usr} se muestra la sección del fichero XDP/arbol1/xdp\_usr.c que implementa las funciones principales previamente descritas.

\CCode[xdp_usr]{XDP Espacio de Usuario}{Fragmento de código que muestra como se carga el objeto, obtiene y fija el mapa y como se adjunta el programa al \textit{hook} XDP}{../XDP/arbol1/xdp_usr.c}{28}{60}{28}

En relación con el código que se ejecuta en el espacio del kernel, en una primera fase la traducción del árbol de decisión se realizaba de manera manual. Es decir, se convertía el árbol entrenado con la librería scikit-learn de Python a código C restringido, con el objetivo de comprender su funcionamiento interno. Una vez adquirido este entendimiento, se implementó la capacidad de autogenerar el código eBPF\/XDP mediante el uso de una plantilla intermedia de Jinja. El proceso consiste en recorrer de manera recursiva el árbol de decisión, sustituyendo cada nodo y sus hijos por condiciones if-else en formato C restringido. A continuación, este código autogenerado se incorpora en una plantilla Jinja que mantiene la estructura del programa eBPF/XDP final, incluyendo las funciones auxiliares y la función main, a excepción del árbol de decisión. Una vez completado este proceso, la plantilla Jinja rellenada se vuelca en el fichero XDP\/arbol\_prueba\/xdp\_kern.c.

Debido a la forma en que funciona el entrenamiento de modelos en la biblioteca scikit-learn, así como a la representación de los datos en lenguaje C, fue necesario realizar un proceso de traducción adicional para la autogeneración del código XDP. Este proceso consistió en adaptar las condiciones presentes en el árbol de decisión al formato requerido por el lenguaje C. Durante el entrenamiento, scikit-learn transforma determinadas variables categóricas en rangos numéricos manejables por el propio programa. Por ejemplo, si una característica puede adoptar tres categorías distintas, el algoritmo genera tres rangos de valores continuos (\verb|0–0.5|, \verb|0.51–1|, \verb|1.01–1.5|, por ejemplo) y asigna a cada categoría un valor específico dentro de dichos rangos, normalmente el límite superior. Asimismo, existe la posibilidad de que, en lugar de utilizar rangos discretos, scikit-learn emplee la condición \verb|<= inf|, lo que en realidad significa que el valor de ese dato simplemente existe. Este último caso se da especialmente en aquellas características cuyo valor no es obligatorio y, por lo tanto, pueden no estar presentes en todos los registros.

En el caso concreto del árbol de decisión preliminar utilizado en el presente trabajo, mostrado en la figura \ref{arbol-decision-preliminar}, se observa la condición \verb|src_port <= inf|, y en pruebas anteriores apareció la condición equivalente \verb|dst_port <= inf|. En ambos casos, dichas condiciones se sustituyen por la expresión \verb#ip_proto == IPPROTO_TCP || ip_proto == IPPROTO_UDP|#, ya que el hecho de que el puerto de origen o destino sea menor que infinito no implica más que la existencia de un puerto, lo cual solo resulta cierto para protocolos como TCP y UDP.

Existen otros elementos que presentan valores binarios, como la comprobación de si el protocolo de transporte corresponde a TCP o UDP. En estos casos, scikit-learn representa la condición mediante rangos numéricos, normalmente \verb|0-0.5| y \verb|0.51-1|. Se ha comprobado que, en este tipo de decisiones booleanas, el rango menor asociado a la condición con el operador ``menor o igual'' (\verb|<=|) se corresponde con el valor lógico ``true'', mientras que el resto representa ``false''. Por ejemplo, cuando el árbol debe verificar si el protocolo de red es TCP, la traducción que se realiza consiste en detectar la aparición de una condición del tipo \verb|protocolo_IP_TCP <=|. En este punto no resulta relevante el rango concreto utilizado, ya que, al tratarse de un valor binario, este puede variar en futuros entrenamientos sin afectar a la validez de la traducción.

La adaptación de estas condiciones al lenguaje C requiere una transformación adicional, ya que en este lenguaje la forma de identificar el protocolo de un paquete se basa en los números de protocolo IP. En el caso de TCP, por ejemplo, el campo correspondiente adopta el valor 6, por lo que la condición se traduce finalmente a \verb|ip_proto == 6|. Una lógica similar se aplica a otros protocolos o campos específicos, como las banderas de IP, que también se representan mediante valores numéricos concretos.

\begin{figure}[Árbol de Decisión Preliminar]{arbol-decision-preliminar}{Árbol de decisión obtenido de un entrenamiento incluyendo el TTL como dato de entrenamiento.}
	\includegraphics[width=0.9\textwidth]{capturas/arbolrw1_mej.png}
\end{figure}

La eliminación del campo \verb|TTL|, junto con la generación del nuevo árbol de decisión que se muestra en la figura \ref{arbol-decision}, introdujo únicamente una regla adicional de traducción del estilo \verb|<= inf|. En este caso, la condición añadida corresponde a \verb|ip_tot_len <= inf|. Esta traducción, al igual que las anteriores, puede observarse en el fragmento de código \ref{traduccionArbol}, donde se recogen de forma conjunta las normas aplicadas para transformar las condiciones producidas por el modelo en expresiones válidas en C restringido.

\begin{figure}[Árbol de Decisión]{arbol-decision}{Árbol de decisión usado para las pruebas de rendimiento.}
	\includegraphics[width=1.15\textwidth]{capturas/arbolrw2.png}
\end{figure}

\PythonCode[traduccionArbol]{Traducción Árbol}{Fragmento de código de arbolrw.py encargado de traducir el árbol de decisión a C.}{../DecisionTree/arbolrw.py}{19}{55}{19}

La fase final del proceso de autogeneración consiste en integrar el árbol de decisión en la plantilla intermedia definida con Jinja mencionada anteriormente. Esta plantilla establece la estructura general del programa eBPF/XDP, en la cual se insertan dinámicamente las reglas de clasificación obtenidas durante el entrenamiento. De este modo, se separa la lógica de decisión —que varía en función del modelo— de la infraestructura del programa, que permanece constante.

En términos de contenido, la plantilla define un programa eBPF/XDP diseñado para inspeccionar paquetes de red en el espacio del kernel y tomar decisiones en función de los valores extraídos de sus cabeceras. En primer lugar, incluye las dependencias necesarias: cabeceras de Linux relacionadas con eBPF, Ethernet, IP, TCP y UDP, además de utilidades para la conversión de endianness. Posteriormente, se declara un mapa BPF de tipo \verb|BPF_MAP_TYPE_ARRAY| denominado \verb|packet_count|, con un máximo de dos entradas, destinado a contabilizar los paquetes según la acción aplicada: permitidos o descartados. Los valores asociados a este mapa se almacenan en contadores de tipo \verb|__u64|.

La plantilla incorpora también un conjunto de funciones auxiliares para el parsing de los paquetes. A nivel de red, se extraen campos como la cabecera IP, el TTL, la longitud total, el identificador, las flags y el offset de fragmentación. A nivel de transporte, se recuperan las cabeceras TCP/UDP junto con los puertos de origen y destino. Todas estas funciones validan los límites de memoria mediante los punteros \verb|data| y \verb|data_end|, garantizando que no se produzcan accesos inválidos.

El núcleo del programa lo constituye la función principal \verb|ransomware_tree|, asociada al \textit{hook} XDP. Esta función recibe cada paquete y determina si debe permitirse su paso (\verb|XDP_PASS|) o descartarse (\verb|XDP_DROP|). De forma predeterminada, el tráfico IPv6 se ignora y se pasa sin procesamiento adicional. La función extrae metadatos como el TTL, la longitud total, el identificador, las flags, el offset, así como parámetros de la cabecera TCP (puertos, número de secuencia, ACK, ventana, tamaño, entre otros). En el bloque identificado como \verb|{{ decision_tree }}| se inserta dinámicamente la lógica del árbol de decisión generada durante el entrenamiento, la cual define el criterio de clasificación del tráfico (por ejemplo, ransomware frente a tráfico legítimo). Finalmente, el mapa \verb|packet_count| se actualiza en función de la acción aplicada, permitiendo monitorizar en todo momento el número de paquetes aceptados y descartados.

En conjunto, esta plantilla actúa como un esqueleto flexible que proporciona toda la estructura necesaria para el filtrado de tráfico en el kernel, a la vez que facilita la integración dinámica del árbol de decisión mediante Jinja. Su diseño modular permite mantener un programa ligero, eficiente y adaptable a distintos escenarios de clasificación de tráfico malicioso. La función principal puede verse en el fragmento de código \ref{xdp_kern_jinja}.

\CCode[xdp_kern_jinja]{Plantilla Jinja XDP espacio de Kernel}{Fragmento de código que muestra la función principal XDP en la plantilla Jinja}{../DecisionTree/xdp_kern.j2}{104}{145}{104}

\section{Compilación y Despliegue}

En la presente sección se describen las herramientas y la preparación del entorno necesarias para la ejecución de Mininet, el entrenamiento del árbol de decisión y la implementación del filtro XDP. Para los entornos de ejecución en Python se ha utilizado la versión \verb|3.10.12|, junto con un entorno virtual creado mediante el comando \verb|python3 -m venv|, en el cual se han instalado, a través del gestor de paquetes \textit{pip}, todas las dependencias requeridas para el desarrollo del proyecto.

La lista completa de paquetes utilizados puede consultarse en el fichero \verb|requirements.txt| incluido en el repositorio de código. Entre las dependencias más relevantes se encuentran \textit{mininet}, \textit{jinja2} y \textit{scikit-learn}, empleadas respectivamente para el manejo de la API de Mininet y la definición de topologías de red, el rellenado de la plantilla Jinja utilizada para la autogeneración del código XDP, y el entrenamiento del árbol de decisión. Adicionalmente, para poder ejecutar Mininet es necesario disponer previamente del comando \textit{mn} instalado en el sistema, ya que este proporciona las utilidades básicas para la creación y gestión de las topologías de red.

Si bien tanto Mininet como el árbol de decisión resultaron relativamente sencillos de trasladar a otros entornos de ejecución, no ocurrió lo mismo con XDP. Esto se debe a que XDP depende de manera directa de múltiples configuraciones del sistema operativo, así como de las versiones concretas de diversas librerías del sistema y de librerías auxiliares empleadas de forma indirecta por estas. En determinados sistemas operativos, algunas de dichas dependencias se encontraban obsoletas o marcadas como deprecated, lo que complicó la portabilidad y la reproducibilidad del entorno de ejecución, especialmente al migrar entre distribuciones o versiones de kernel distintas.

Para mitigar estas complicaciones, se optó por desarrollar un Makefile dinámico, el cual puede consultarse en el fichero \verb|XDP/arbol_prueba| del repositorio. Este Makefile comienza comprobando la arquitectura del sistema mediante el comando \verb|$(shell uname -m)|, lo que permite asignar posteriormente, a través de estructuras condicionales, el valor adecuado a la variable \verb|BPF_ARCH|. Dicha variable se emplea más adelante para definir correctamente las flags de compilación específicas de la arquitectura.

Antes de iniciar el proceso de compilación propiamente dicho, el Makefile genera de forma dinámica las variables \textit{CFLAGS} y \textit{LIBS}. Para ello, se utilizan los comandos \verb|$(shell pkg-config --cflags| \verb|libbpf 2>/dev/null)| y \verb|$(shell pkg-config --libs libbpf 2>/dev/null) -ldl|, respectivamente. Este enfoque resulta especialmente relevante, ya que el código ha sido adaptado para ser compatible con distintas versiones de \verb|libbpf|, la librería encargada de proporcionar la API de eBPF/XDP, facilitando así la portabilidad y la correcta compilación en entornos heterogéneos.

El proceso de despliegue de la arquitectura sigue una secuencia bien definida. En primer lugar, se debe activar el entorno virtual de Python, garantizando así que todas las dependencias necesarias se encuentren disponibles en el entorno de ejecución. A continuación, se ejecuta el script en Python encargado del entrenamiento del árbol de decisión y de la posterior generación del código XDP, proceso que culmina con la creación del código fuente mediante el uso de la plantilla Jinja.

Una vez generado el código XDP, se procede a ejecutar el Makefile, el cual, apoyándose en herramientas como \textit{gcc}, \textit{Clang} y \textit{LLVM}, compila los distintos componentes del sistema, generando el ejecutable del programa de espacio de usuario (\verb|xdp_usr|), el fichero objeto correspondiente al programa del kernel (\verb|xdp_kern.o|) y el bytecode asociado a \verb|xdp_kern|. Finalmente, manteniendo activo el entorno virtual de Python, se lanza mediante un script Python el entorno de Mininet, desde el cual se pueden ejecutar las distintas pruebas y experimentos definidos en el presente trabajo.
